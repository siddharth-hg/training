{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be863ed3",
   "metadata": {},
   "source": [
    "Scenario: Employee Promotion Decision (Single-Layer Perceptron)\n",
    "Inputs (2 features):\n",
    "Years of experience (0 = low, 1 = sufficient)\n",
    "Performance rating (0 = poor, 1 = good)\n",
    "Output:\n",
    "1 = gets promoted\n",
    "0 = not promoted\n",
    "Rule (intuitive):\n",
    "Employee gets promoted only if both experience and performance are good.\n",
    "This is again an AND logic gate, but in a workplace context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e466edf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating our own synthetic dataset.\n",
    "import numpy as np\n",
    " \n",
    "# We are creating a dataset for a promotion decision problem.\n",
    "# Features (X):\n",
    "#   X[:,0] -> experience (0 = low, 1 = sufficient)\n",
    "#   X[:,1] -> performance (0 = poor, 1 = good)\n",
    "#\n",
    "# Target (y):\n",
    "#   1 = employee gets promoted\n",
    "#   0 = employee does NOT get promoted\n",
    "#\n",
    "# Rule used:\n",
    "#   Promotion happens ONLY when BOTH experience=1 AND performance=1.\n",
    "#   This is logically the AND gate.\n",
    "#\n",
    "# Below we repeat each type 2-3 times to make the dataset slightly larger.\n",
    " \n",
    "# X = input matrix (experience, performance)\n",
    "X = np.array([\n",
    "    [0, 0],   # low experience, poor performance â†’ not promoted\n",
    "    [0, 0],   # repeated sample for training stability\n",
    "    [0, 1],   # low experience, good performance â†’ still not promoted\n",
    "    [0, 1],   # repeated\n",
    "    [1, 0],   # sufficient experience, poor performance â†’ not promoted\n",
    "    [1, 0],   # repeated\n",
    "    [1, 1],   # sufficient experience, good performance â†’ promoted\n",
    "    [1, 1],   # repeated\n",
    "    [1, 1],   # repeated again (multiple positives)\n",
    "])\n",
    " \n",
    "# y = output labels (promotion decision)\n",
    "y = np.array([\n",
    "    [0],  # employee not promoted\n",
    "    [0],  # not promoted (repeat)\n",
    "    [0],  # not promoted (good performance but low experience)\n",
    "    [0],  # not promoted (repeat)\n",
    "    [0],  # not promoted (experience OK but performance poor)\n",
    "    [0],  # not promoted (repeat)\n",
    "    [1],  # promoted (both conditions satisfied)\n",
    "    [1],  # promoted (repeat)\n",
    "    [1],  # promoted (repeat)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bece5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2602\n",
      "Epoch 1000, Loss: 0.0129\n",
      "Epoch 2000, Loss: 0.0034\n",
      "Epoch 3000, Loss: 0.0018\n",
      "Epoch 4000, Loss: 0.0012\n",
      "\n",
      "Predictions after training:\n",
      "[0 0] -> 0.010 (Target: 0)\n",
      "[0 0] -> 0.010 (Target: 0)\n",
      "[0 1] -> 0.033 (Target: 0)\n",
      "[0 1] -> 0.033 (Target: 0)\n",
      "[1 0] -> 0.034 (Target: 0)\n",
      "[1 0] -> 0.034 (Target: 0)\n",
      "[1 1] -> 0.966 (Target: 1)\n",
      "[1 1] -> 0.966 (Target: 1)\n",
      "[1 1] -> 0.966 (Target: 1)\n"
     ]
    }
   ],
   "source": [
    "# 1. Activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "\n",
    "#initializing hidden weights, inputs and biases.\n",
    "np.random.seed(1)\n",
    "weights_input_hidden = np.random.rand(2,2)\n",
    "weights_hidden_output = np.random.rand(2,1)\n",
    "bias_hidden = np.zeros((1,2))\n",
    "bias_output = np.zeros((1,1))\n",
    "\n",
    "#training settings\n",
    "epochs = 5000\n",
    "learning_rate = 0.1\n",
    "\n",
    "#training of the model\n",
    "for epoch in range(epochs):\n",
    "    #forward pass\n",
    "    # Hidden layer\n",
    "    hidden_input = np.dot(X, weights_input_hidden) + bias_hidden\n",
    "    hidden_output = sigmoid(hidden_input)\n",
    "    \n",
    "    # Output layer\n",
    "    final_input = np.dot(hidden_output, weights_hidden_output) + bias_output\n",
    "    final_output = sigmoid(final_input)\n",
    "    # ---- Error ----\n",
    "    error = y - final_output\n",
    "\n",
    "    # ---- Backpropagation ----\n",
    "    d_output = error * sigmoid_derivative(final_output)\n",
    "    d_hidden = d_output.dot(weights_hidden_output.T) * sigmoid_derivative(hidden_output)\n",
    "\n",
    "    # ---- Update weights and biases ----\n",
    "    weights_hidden_output += hidden_output.T.dot(d_output) * learning_rate\n",
    "    bias_output += np.sum(d_output, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    weights_input_hidden += X.T.dot(d_hidden) * learning_rate\n",
    "    bias_hidden += np.sum(d_hidden, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    # Print loss sometimes\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "\n",
    "# 5. Predictions after training\n",
    "print(\"\\nPredictions after training:\")\n",
    "for i, inputs in enumerate(X):\n",
    "    hidden_output = sigmoid(np.dot(inputs, weights_input_hidden) + bias_hidden)\n",
    "    final_output = sigmoid(np.dot(hidden_output, weights_hidden_output) + bias_output)\n",
    "    print(f\"{inputs} -> {final_output[0][0]:.3f} (Target: {y[i][0]})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374127e6",
   "metadata": {},
   "source": [
    "    Scenario: Loan Approval Prediction (Multilayer Neural Network)\n",
    "    Context\n",
    "    Banks want to decide whether to approve a loan application. The decision depends on multiple factors, and the relationships are non-linear (not just a simple rule).\n",
    "    Inputs (Features)\n",
    "    Income level (normalized numeric value)\n",
    "    Credit score (normalized numeric value)\n",
    "    Output\n",
    "    1 = Loan approved\n",
    "    0 = Loan rejected\n",
    "    Rule (intuitive, hidden from model)\n",
    "    Higher income + good credit score â†’ likely approved.\n",
    "    Low income + poor credit score â†’ likely rejected.\n",
    "    Middle cases depend on combinations (non-linear patterns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc98faaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 74.42%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "\n",
    "# ------------------ 2) Synthetic Data ------------------\n",
    "N = 8000\n",
    "income = np.clip(np.random.beta(a=2.0, b=2.0, size=N), 0, 1)\n",
    "credit = np.clip(np.random.beta(a=2.5, b=1.8, size=N), 0, 1)\n",
    "X = np.c_[income, credit]\n",
    "\n",
    "\n",
    "def sigmoid(z): return 1 / (1 + np.exp(-z))\n",
    "z = (\n",
    "    3.0 * (credit - 0.6) +\n",
    "    2.0 * (income - 0.5) +\n",
    "    3.5 * (income * credit - 0.3) +\n",
    "    1.2 * (credit**2 - 0.35)\n",
    ")\n",
    "p_approve = sigmoid(z)\n",
    "y = (np.random.rand(N) < p_approve).astype(int)\n",
    "\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ------------------ 3) Build MLP ------------------\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(2,)),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(8, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=64,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test accuracy: {acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e0ad1a",
   "metadata": {},
   "source": [
    "NEW CONTEXT: HOSPITAL PATIENT DETERIORATION PREDICTION\n",
    "Scenario: ICU Patient Health Risk Forecasting\n",
    "A hospital ICU continuously monitors critical patient vitals.\n",
    "The goal is to predict the patientâ€™s risk score for the next 6 hours using the previous 12 hours of multivariate time-series data, so doctors can intervene before a medical emergency occurs.\n",
    "ğŸ”¹ Parameters Collected (Every Hour)\n",
    "Feature\tDescription\n",
    "heart_rate\tBeats per minute\n",
    "systolic_bp\tBlood pressure (mmHg)\n",
    "oxygen\tOxygen saturation (%)\n",
    "resp_rate\tBreaths per minute\n",
    "temperature\tBody temperature (Â°C)\n",
    "ğŸ”¹ Sample Dataset (12 Hours)\n",
    " \n",
    "Hour\tHR\tBP\tOâ‚‚\tResp\tTemp\n",
    "1\t82\t120\t98\t16\t36.8\n",
    "2\t85\t118\t97\t17\t36.9\n",
    "3\t88\t115\t96\t18\t37.0\n",
    "4\t92\t112\t95\t19\t37.1\n",
    "5\t96\t108\t94\t20\t37.2\n",
    "6\t100\t105\t93\t21\t37.4\n",
    "7\t104\t102\t92\t22\t37.6\n",
    "8\t108\t98\t91\t23\t37.8\n",
    "9\t112\t95\t90\t24\t38.0\n",
    "10\t118\t92\t89\t26\t38.3\n",
    "11\t122\t88\t87\t28\t38.6\n",
    "12\t128\t85\t85\t30\t39.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba49c9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input shape: (1, 12, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Siddharth.Kshirsagar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted risk for next 6 hours (0-1):\n",
      "t+1h: 0.398\n",
      "t+2h: 0.369\n",
      "t+3h: 0.555\n",
      "t+4h: 0.441\n",
      "t+5h: 0.578\n",
      "t+6h: 0.540\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =========================\n",
    "# ICU Risk Forecast (RNN)\n",
    "# =========================\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"Datasets/ICU_12h_vitals.csv\")  # ensure the file is in your working directory\n",
    "\n",
    "# 2) Select the 5 vital features (exclude Hour)\n",
    "features = [\"HR\", \"BP\", \"O2\", \"Resp\", \"Temp\"]\n",
    "X_df = df[features].copy()\n",
    "\n",
    "# 3) (Optional) Scale features to [0,1] using approximate clinical ranges\n",
    "#    Adjust ranges if you have site-specific norms.\n",
    "range_mins = np.array([40, 70, 70, 8, 34.0], dtype=float)   # HR, BP, O2, Resp, Temp\n",
    "range_maxs = np.array([160, 180, 100, 40, 41.0], dtype=float)\n",
    "\n",
    "X_norm = (X_df.values - range_mins) / (range_maxs - range_mins)\n",
    "X_norm = np.clip(X_norm, 0.0, 1.0)\n",
    "\n",
    "# 4) Reshape to (batch, time_steps, features) = (1,12,5)\n",
    "X_input = X_norm.reshape(1, 12, 5)\n",
    "\n",
    "print(\"Model input shape:\", X_input.shape)  # (1, 12, 5)\n",
    "\n",
    "\n",
    "# --- Build a tiny SimpleRNN model ---\n",
    "model = models.Sequential([\n",
    "    layers.SimpleRNN(32, activation='tanh', input_shape=(12, 5)),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(6, activation='sigmoid')  # 6 risk scores in [0,1]\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "y_dummy = np.random.rand(1, 6)  # placeholder; delete when you have labels\n",
    "model.fit(X_input, y_dummy, epochs=1, verbose=0)\n",
    "\n",
    "# --- Predict next 6-hour risk ---\n",
    "pred = model.predict(X_input, verbose=0).flatten()\n",
    "\n",
    "print(\"Predicted risk for next 6 hours (0-1):\")\n",
    "for i, r in enumerate(pred, start=1):\n",
    "    print(f\"t+{i}h: {r:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe5eabf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 962ms/step - loss: 0.0455 - mae: 0.1815\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0412 - mae: 0.1714\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0377 - mae: 0.1626\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0356 - mae: 0.1573\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0336 - mae: 0.1522\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0311 - mae: 0.1454\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0287 - mae: 0.1388\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0265 - mae: 0.1325\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0245 - mae: 0.1272\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0227 - mae: 0.1224\n",
      "History keys: dict_keys(['loss', 'mae'])\n",
      "Epoch 1: loss=0.0455, mae=0.1815\n",
      "Epoch 2: loss=0.0412, mae=0.1714\n",
      "Epoch 3: loss=0.0377, mae=0.1626\n",
      "Epoch 4: loss=0.0356, mae=0.1573\n",
      "Epoch 5: loss=0.0336, mae=0.1522\n",
      "Epoch 6: loss=0.0311, mae=0.1454\n",
      "Epoch 7: loss=0.0287, mae=0.1388\n",
      "Epoch 8: loss=0.0265, mae=0.1325\n",
      "Epoch 9: loss=0.0245, mae=0.1272\n",
      "Epoch 10: loss=0.0227, mae=0.1224\n"
     ]
    }
   ],
   "source": [
    "# Assume X_input is shaped (1, 12, 5) from your CSV preprocessing\n",
    "# and you have some training windows later; for demo we create dummy targets\n",
    "X_train = X_input  # (1,12,5) â€“ demo only\n",
    "y_train = np.random.rand(1, 6).astype(\"float32\")  # demo target for 6 hours\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.SimpleRNN(32, activation='tanh', input_shape=(12, 5)),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(6, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile with regression metrics\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=[tf.keras.metrics.MeanAbsoluteError(name='mae')]\n",
    ")\n",
    "\n",
    "# Use validation_split (if you have >1 sample) or provide validation_data\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=1,\n",
    "    validation_split=0.0,  # set >0 when you have enough samples\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Print final metrics\n",
    "print(\"History keys:\", history.history.keys())   # contains 'loss' and 'mae'; add val_* when you have validation\n",
    "for epoch, (loss, mae) in enumerate(zip(history.history['loss'], history.history['mae']), 1):\n",
    "    print(f\"Epoch {epoch}: loss={loss:.4f}, mae={mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b3cf8e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,352</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚         \u001b[38;5;34m4,352\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m33\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,385</span> (17.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,385\u001b[0m (17.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,385</span> (17.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,385\u001b[0m (17.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM final train loss: 0.0004062442167196423\n",
      "LSTM final val loss  : 0.010700535960495472\n",
      "GRU final train loss: 0.0001427667011739686\n",
      "GRU final val loss  : 0.010459857992827892\n",
      "Last 7 days traffic: [1141.4 1126.  1132.5 1156.8 1181.1 1187.6 1172.2]\n",
      "LSTM predicted next day traffic: 1145.9 visits\n",
      "GRU  predicted next day traffic: 1142.7 visits\n"
     ]
    }
   ],
   "source": [
    "# Scenario and use case\n",
    "# Scenario:\n",
    "# Predict next dayâ€™s website traffic (visits) from the last 7 days to help plan server capacity.\n",
    "\n",
    "# Typical LSTM/GRU uses:\n",
    "\n",
    "# Time series: stock prices, traffic, energy demand.\n",
    "\n",
    "\n",
    "# NLP: next-word prediction, translation, sentiment.\n",
    "\n",
    "\n",
    "# Sequences: clickstreams, sensor readings, speech.\n",
    "\n",
    "\n",
    "# 1. Data preparation\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Create synthetic traffic data\n",
    "# -----------------------------\n",
    "days = np.arange(200)\n",
    "# baseline 1000 visits, weekly seasonality + small upward trend\n",
    "traffic = 1000 + 30 * np.sin(2 * np.pi * days / 7) + 0.8 * days\n",
    "\n",
    "window_size = 7   # use last 7 days -> predict next day\n",
    "\n",
    "X_list, y_list = [], []\n",
    "for i in range(len(traffic) - window_size):\n",
    "    X_list.append(traffic[i:i + window_size])\n",
    "    y_list.append(traffic[i + window_size])\n",
    "\n",
    "X = np.array(X_list)            # (samples, 7)\n",
    "y = np.array(y_list)            # (samples,)\n",
    "\n",
    "# Normalize (simple)\n",
    "mean = X.mean()\n",
    "std = X.std()\n",
    "X_norm = (X - mean) / std\n",
    "y_norm = (y - mean) / std\n",
    "\n",
    "# Reshape for RNN: (batch, time_steps, features)\n",
    "X_norm = X_norm[..., np.newaxis]   # (samples, 7, 1)\n",
    "y_norm = y_norm[..., np.newaxis]   # (samples, 1)\n",
    "\n",
    "# Train/validation split\n",
    "split = int(0.8 * len(X_norm))\n",
    "X_train, X_val = X_norm[:split], X_norm[split:]\n",
    "y_train, y_val = y_norm[:split], y_norm[split:]\n",
    "\n",
    "# This creates a realistic-looking daily traffic series with weekly seasonality and a trend.\n",
    "\n",
    "# 2. LSTM architecture and working model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Build LSTM model\n",
    "# -----------------------------\n",
    "def build_lstm_model():\n",
    "    model = Sequential([\n",
    "        LSTM(\n",
    "            units=32,\n",
    "            activation='tanh',\n",
    "            recurrent_activation='sigmoid',\n",
    "            input_shape=(window_size, 1)\n",
    "        ),\n",
    "        Dense(1)  # predict normalized next-day traffic\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "lstm_model = build_lstm_model()\n",
    "lstm_model.summary()\n",
    "\n",
    "# LSTM cell behaviour (intuitive):\n",
    "\n",
    "# Maintains a cell state that flows along time steps, plus a hidden state.\n",
    "\n",
    "\n",
    "# Uses input, forget, and output gates to decide\n",
    "\n",
    "# what new information to add,\n",
    "\n",
    "# what to erase,\n",
    "\n",
    "# what to output at each time step.\n",
    "\n",
    "\n",
    "# This design helps keep important gradients from vanishing across many steps.\n",
    "\n",
    "# 3. Train LSTM (optimizer + learning rate)\n",
    "# -----------------------------\n",
    "# 3. Compile with Adam optimizer\n",
    "# -----------------------------\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)  # adaptive GD [web:51]\n",
    "\n",
    "lstm_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Train the model\n",
    "# -----------------------------\n",
    "history_lstm = lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=40,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"LSTM final train loss:\", history_lstm.history['loss'][-1])\n",
    "print(\"LSTM final val loss  :\", history_lstm.history['val_loss'][-1])\n",
    "# Adam performs gradient descent with adaptive learning rates and momentum, well suited for LSTM training.\n",
    "\n",
    "\n",
    "# Learning rate 0.005 balances stability and speed; adjusting it changes convergence behaviour.\n",
    "\n",
    "# 4. GRU model on the same data\n",
    "\n",
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Build GRU model\n",
    "# -----------------------------\n",
    "def build_gru_model():\n",
    "    model = Sequential([\n",
    "        GRU(\n",
    "            units=32,\n",
    "            activation='tanh',\n",
    "            recurrent_activation='sigmoid',\n",
    "            input_shape=(window_size, 1)\n",
    "        ),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "gru_model = build_gru_model()\n",
    "\n",
    "gru_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.005),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "history_gru = gru_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=40,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"GRU final train loss:\", history_gru.history['loss'][-1])\n",
    "print(\"GRU final val loss  :\", history_gru.history['val_loss'][-1])\n",
    "\n",
    "# GRU uses update and reset gates with a single hidden state, merging some LSTM gates.\n",
    "\n",
    "\n",
    "# For the same units, GRU has fewer parameters and is usually faster, often with similar accuracy.\n",
    "\n",
    "\n",
    "# 5. Prediction and LSTM vs GRU comparison\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Predict next day's traffic\n",
    "# -----------------------------\n",
    "last_week = traffic[-window_size:]\n",
    "last_week_norm = ((last_week - mean) / std).reshape(1, window_size, 1)\n",
    "\n",
    "pred_lstm_norm = lstm_model.predict(last_week_norm, verbose=0)[0, 0]\n",
    "pred_gru_norm = gru_model.predict(last_week_norm, verbose=0)[0, 0]\n",
    "\n",
    "pred_lstm = pred_lstm_norm * std + mean\n",
    "pred_gru = pred_gru_norm * std + mean\n",
    "\n",
    "print(\"Last 7 days traffic:\", np.round(last_week, 1))\n",
    "print(f\"LSTM predicted next day traffic: {pred_lstm:.1f} visits\")\n",
    "print(f\"GRU  predicted next day traffic: {pred_gru:.1f} visits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35462f38",
   "metadata": {},
   "source": [
    "ğŸ“Œ Business Scenario\n",
    "A smart city power department wants to predict tomorrowâ€™s electricity demand (in MW) using the last 7 days of hourly consumption data.\n",
    "This helps to:\n",
    "Avoid power shortages âš¡\n",
    "Optimize power generation scheduling\n",
    "Reduce operational costs\n",
    "ğŸ”¹ Sample Dataset (Simplified â€“ Daily Average)\n",
    " \n",
    "Day\tElectricity Demand (MW)\n",
    "1\t320\n",
    "2\t340\n",
    "3\t360\n",
    "4\t355\n",
    "5\t370\n",
    "6\t390\n",
    "7\t410\n",
    "8\tTarget (to predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd7acf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: (0, 100, 1)  Test samples: (0, 100, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Siddharth.Kshirsagar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Training data contains 0 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.2`. Either provide more data, or a different value for the `validation_split` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 94\u001b[0m\n\u001b[0;32m     90\u001b[0m lstm \u001b[38;5;241m=\u001b[39m build_lstm()\n\u001b[0;32m     91\u001b[0m gru  \u001b[38;5;241m=\u001b[39m build_gru()\n\u001b[1;32m---> 94\u001b[0m hist_lstm \u001b[38;5;241m=\u001b[39m \u001b[43mlstm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m hist_gru \u001b[38;5;241m=\u001b[39m gru\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    103\u001b[0m     X_train, y_train,\n\u001b[0;32m    104\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    107\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    108\u001b[0m )\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# --------------------------\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# 7) Evaluate on TEST (metrics on original MW scale)\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# --------------------------\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Siddharth.Kshirsagar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Siddharth.Kshirsagar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_slicing.py:500\u001b[0m, in \u001b[0;36mtrain_validation_split\u001b[1;34m(arrays, validation_split)\u001b[0m\n\u001b[0;32m    497\u001b[0m split_at \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(math\u001b[38;5;241m.\u001b[39mfloor(batch_dim \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m validation_split)))\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split_at \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m split_at \u001b[38;5;241m==\u001b[39m batch_dim:\n\u001b[1;32m--> 500\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    501\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining data contains \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples, which is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    502\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msufficient to split it into a validation and training set as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecified by `validation_split=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalidation_split\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. Either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprovide more data, or a different value for the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`validation_split` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    506\u001b[0m     )\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_split\u001b[39m(t, start, end):\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Training data contains 0 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.2`. Either provide more data, or a different value for the `validation_split` argument."
     ]
    }
   ],
   "source": [
    "\n",
    "# ===========================\n",
    "# Smart City Demand Forecast\n",
    "# LSTM & GRU\n",
    "# ===========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "\n",
    "# --------------------------\n",
    "# 1) Load the 100-day dataset\n",
    "# --------------------------\n",
    "df = pd.read_csv(\"Datasets/smartcity_power_100days.csv\")\n",
    "series = df[\"Electricity_Demand_MW\"].astype(float).values  # shape: (100,)\n",
    "\n",
    "# --------------------------\n",
    "# 2) Train/Test split (chronological)\n",
    "#    Use first 85 days to train/validate, last 15 days to test\n",
    "# --------------------------\n",
    "N = len(series)                 # 100\n",
    "train_size = 85\n",
    "train_series = series[:train_size].reshape(-1, 1)\n",
    "test_series  = series[train_size:].reshape(-1, 1)\n",
    "\n",
    "# --------------------------\n",
    "# 3) Scale to [0,1] using train-only fit (avoid leakage)\n",
    "# --------------------------\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_series)\n",
    "series_scaled = scaler.transform(series.reshape(-1, 1)).flatten()  # scaled entire series (for windowing)\n",
    "\n",
    "# --------------------------\n",
    "# 4) Make sliding windows: last 7 days -> next day\n",
    "# --------------------------\n",
    "WINDOW = 100\n",
    "\n",
    "def make_windows(arr, win=100):\n",
    "    X, y = [], []\n",
    "    # arr is 1D scaled series\n",
    "    for t in range(win, len(arr)):\n",
    "        hist = arr[t-win:t]     # last 7 values\n",
    "        target = arr[t]         # next value\n",
    "        X.append(hist)\n",
    "        y.append(target)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_all, y_all = make_windows(series_scaled, WINDOW)  # X_all: (N-7, 7), y_all: (N-7,)\n",
    "\n",
    "# Indices of each sample correspond to targets at positions [7..N-1]\n",
    "target_indices = np.arange(WINDOW, N)\n",
    "\n",
    "# Train samples are those whose target day < train_size\n",
    "train_mask = target_indices < train_size\n",
    "test_mask  = target_indices >= train_size\n",
    "\n",
    "X_train, y_train = X_all[train_mask], y_all[train_mask]\n",
    "X_test,  y_test  = X_all[test_mask],  y_all[test_mask]\n",
    "\n",
    "# Reshape for RNN: (samples, time_steps, features)\n",
    "X_train = X_train.reshape(-1, WINDOW, 1)\n",
    "X_test  = X_test.reshape(-1, WINDOW, 1)\n",
    "\n",
    "print(\"Train samples:\", X_train.shape, \" Test samples:\", X_test.shape)\n",
    "\n",
    "# --------------------------\n",
    "# 5) Build two tiny models: LSTM and GRU\n",
    "# --------------------------\n",
    "def build_lstm():\n",
    "    model = models.Sequential([\n",
    "        layers.LSTM(32, input_shape=(WINDOW, 1)),\n",
    "        layers.Dense(16, activation=\"relu\"),\n",
    "        layers.Dense(1)  # regression\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\",\n",
    "                  metrics=[tf.keras.metrics.MeanAbsoluteError(name=\"mae\")])\n",
    "    return model\n",
    "\n",
    "def build_gru():\n",
    "    model = models.Sequential([\n",
    "        layers.GRU(32, input_shape=(WINDOW, 1)),\n",
    "        layers.Dense(16, activation=\"relu\"),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\",\n",
    "                  metrics=[tf.keras.metrics.MeanAbsoluteError(name=\"mae\")])\n",
    "    return model\n",
    "\n",
    "lstm = build_lstm()\n",
    "gru  = build_gru()\n",
    "\n",
    "hist_lstm = lstm.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=8,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "hist_gru = gru.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=8,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# 7) Evaluate on TEST (metrics on original MW scale)\n",
    "# --------------------------\n",
    "def inverse_scale(arr_1d):\n",
    "    # arr_1d is shape (n,) or (n,1) in scaled space -> back to MW\n",
    "    return scaler.inverse_transform(np.array(arr_1d).reshape(-1,1)).flatten()\n",
    "\n",
    "# Predictions on test set\n",
    "yhat_lstm_test = lstm.predict(X_test, verbose=0).flatten()\n",
    "yhat_gru_test  = gru.predict(X_test,  verbose=0).flatten()\n",
    "\n",
    "# Inverse transform to MW\n",
    "y_test_MW       = inverse_scale(y_test)\n",
    "yhat_lstm_MW    = inverse_scale(yhat_lstm_test)\n",
    "yhat_gru_MW     = inverse_scale(yhat_gru_test)\n",
    "\n",
    "# Metrics\n",
    "def mae(a, b):   return np.mean(np.abs(a - b))\n",
    "def rmse(a, b):  return math.sqrt(np.mean((a - b)**2))\n",
    "def mape(a, b):  return np.mean(np.abs((a - b) / (np.clip(a, 1e-8, None)))) * 100\n",
    "\n",
    "print(\"\\n=== TEST METRICS (Original MW scale) ===\")\n",
    "print(f\"LSTM -> MAE: {mae(y_test_MW, yhat_lstm_MW):.2f} | RMSE: {rmse(y_test_MW, yhat_lstm_MW):.2f} | MAPE: {mape(y_test_MW, yhat_lstm_MW):.2f}%\")\n",
    "print(f\"GRU  -> MAE: {mae(y_test_MW, yhat_gru_MW):.2f}  | RMSE: {rmse(y_test_MW, yhat_gru_MW):.2f}  | MAPE: {mape(y_test_MW, yhat_gru_MW):.2f}%\")\n",
    "\n",
    "# --------------------------\n",
    "# 8) Print training history (loss & val_loss)\n",
    "# --------------------------\n",
    "def print_history(name, h):\n",
    "    last_epoch = len(h.history[\"loss\"])\n",
    "    print(f\"\\n[{name}] epochs trained: {last_epoch}\")\n",
    "    for k in h.history.keys():\n",
    "        if k.startswith(\"val_\"):\n",
    "            continue\n",
    "    # print last values\n",
    "    print(f\"[{name}] final loss: {h.history['loss'][-1]:.6f} | final val_loss: {h.history['val_loss'][-1]:.6f}\")\n",
    "    if \"mae\" in h.history:\n",
    "        print(f\"[{name}] final mae:  {h.history['mae'][-1]:.6f}  | final val_mae: {h.history['val_mae'][-1]:.6f}\")\n",
    "\n",
    "print_history(\"LSTM\", hist_lstm)\n",
    "print_history(\"GRU\",  hist_gru)\n",
    "\n",
    "# --------------------------\n",
    "# 9) Predict Day 101\n",
    "#    Use the last 7 days (Days 94..100) as input\n",
    "# --------------------------\n",
    "last7_scaled = series_scaled[-WINDOW:].reshape(1, WINDOW, 1)\n",
    "\n",
    "pred101_lstm_scaled = lstm.predict(last7_scaled, verbose=0)\n",
    "pred101_gru_scaled  = gru.predict(last7_scaled,  verbose=0)\n",
    "\n",
    "pred101_lstm = inverse_scale(pred101_lstm_scaled)[0]\n",
    "pred101_gru  = inverse_scale(pred101_gru_scaled)[0]\n",
    "pred101_avg  = (pred101_lstm + pred101_gru) / 2.0\n",
    "\n",
    "print(\"\\n=== FORECAST: Day 101 ===\")\n",
    "print(f\"LSTM prediction (MW): {pred101_lstm:.2f}\")\n",
    "print(f\"GRU  prediction (MW): {pred101_gru:.2f}\")\n",
    "print(f\"Ensemble average     : {pred101_avg:.2f}\")\n",
    "\n",
    "# (Optional) Save forecasts\n",
    "out = pd.DataFrame({\n",
    "    \"model\": [\"LSTM\", \"GRU\", \"Avg(LSTM,GRU)\"],\n",
    "    \"day\": [101, 101, 101],\n",
    "    \"prediction_MW\": [pred101_lstm, pred101_gru, pred101_avg]\n",
    "})\n",
    "out.to_csv(\"day101_forecasts.csv\", index=False)\n",
    "print(\"\\nSaved: day101_forecasts.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c27fb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: module 'importlib.metadata' has no attribute 'packages_distributions'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Siddharth.Kshirsagar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\api_core\\_python_version_support.py:237: FutureWarning: You are using a non-supported Python version (3.9.7). Google will not post any further updates to google.api_core supporting this Python version. Please upgrade to the latest Python version, or at least Python 3.10, and then update google.api_core.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\Users\\Siddharth.Kshirsagar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\auth\\__init__.py:54: FutureWarning: \n",
      "    You are using a Python version 3.9 past its end of life. Google will update\n",
      "    google-auth with critical bug fixes on a best-effort basis, but not\n",
      "    with any other fixes or features. Please upgrade your Python version,\n",
      "    and then update google-auth.\n",
      "    \n",
      "  warnings.warn(eol_message.format(\"3.9\"), FutureWarning)\n",
      "c:\\Users\\Siddharth.Kshirsagar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\oauth2\\__init__.py:40: FutureWarning: \n",
      "    You are using a Python version 3.9 past its end of life. Google will update\n",
      "    google-auth with critical bug fixes on a best-effort basis, but not\n",
      "    with any other fixes or features. Please upgrade your Python version,\n",
      "    and then update google-auth.\n",
      "    \n",
      "  warnings.warn(eol_message.format(\"3.9\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
      "[('pythons', 0.66883784532547), ('Burmese_python', 0.6680365800857544), ('snake', 0.6606292724609375), ('crocodile', 0.6591362953186035), ('boa_constrictor', 0.6443520188331604)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 1. Set the download directory to D drive BEFORE importing gensim.downloader\n",
    "os.environ['GENSIM_DATA_DIR'] = r'D:\\gensim-data'\n",
    " \n",
    "import gensim.downloader as api\n",
    " \n",
    "# 2. This will now download the 1.6GB model to D:\\gensim-data\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    " \n",
    "# Rest of your code\n",
    "vector = model['python']\n",
    "similar = model.most_similar('python', topn=5)\n",
    "print(similar)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb5e0a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('China', 0.5742051601409912), ('Pakistan', 0.5350226163864136), ('Bangladesh', 0.5193105340003967), ('erstwhile_USSR', 0.5165647268295288), ('IndiaÃ¢_â‚¬_â„¢', 0.5160695314407349)]\n"
     ]
    }
   ],
   "source": [
    "similar = model.most_similar(positive=['Russia', 'India'],\n",
    "    negative=['Trump'], topn=5)\n",
    "# Returns: [('Washington', 0.68)]\n",
    "print(similar)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e2cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [['Machine','learning','is','fun'],['deep','learning','is','bad']]\n",
    "\n",
    "model = Word2Vec(sentences,sg=1,vector_size=100, window=5,min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "213ad819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (77, 7, 1) y_train: (77,)\n",
      "X_val  : (11, 7, 1) y_val  : (11,)\n",
      "X_test : (11, 7, 1) y_test : (11,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Siddharth.Kshirsagar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 183ms/step - loss: 0.3941 - val_loss: 0.2942\n",
      "Epoch 2/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 0.1651 - val_loss: 0.1146\n",
      "Epoch 3/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0806 - val_loss: 0.1418\n",
      "Epoch 4/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0991 - val_loss: 0.1149\n",
      "Epoch 5/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0759 - val_loss: 0.1168\n",
      "Epoch 6/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0782 - val_loss: 0.1247\n",
      "Epoch 7/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0729 - val_loss: 0.1194\n",
      "Epoch 8/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0913 - val_loss: 0.1127\n",
      "Epoch 9/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0821 - val_loss: 0.1099\n",
      "Epoch 10/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0823 - val_loss: 0.1097\n",
      "Epoch 11/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0687 - val_loss: 0.1102\n",
      "Epoch 12/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0780 - val_loss: 0.1107\n",
      "Epoch 13/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0746 - val_loss: 0.1110\n",
      "Epoch 14/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0802 - val_loss: 0.1091\n",
      "Epoch 15/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0846 - val_loss: 0.1066\n",
      "Epoch 16/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0701 - val_loss: 0.1060\n",
      "Epoch 17/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0657 - val_loss: 0.1073\n",
      "Epoch 18/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0703 - val_loss: 0.1094\n",
      "Epoch 19/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0648 - val_loss: 0.1048\n",
      "Epoch 20/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0710 - val_loss: 0.1008\n",
      "Epoch 21/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0675 - val_loss: 0.1007\n",
      "Epoch 22/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0627 - val_loss: 0.1023\n",
      "Epoch 23/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0646 - val_loss: 0.0995\n",
      "Epoch 24/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0749 - val_loss: 0.0950\n",
      "Epoch 25/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0673 - val_loss: 0.0934\n",
      "Epoch 26/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0634 - val_loss: 0.0956\n",
      "Epoch 27/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0577 - val_loss: 0.0966\n",
      "Epoch 28/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0582 - val_loss: 0.0873\n",
      "Epoch 29/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0631 - val_loss: 0.0837\n",
      "Epoch 30/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0583 - val_loss: 0.0908\n",
      "Epoch 31/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0552 - val_loss: 0.0850\n",
      "Epoch 32/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0616 - val_loss: 0.0746\n",
      "Epoch 33/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0673 - val_loss: 0.0722\n",
      "Epoch 34/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0488 - val_loss: 0.0691\n",
      "Epoch 35/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0475 - val_loss: 0.0626\n",
      "Epoch 36/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0484 - val_loss: 0.0589\n",
      "Epoch 37/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0376 - val_loss: 0.0553\n",
      "Epoch 38/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0458 - val_loss: 0.0446\n",
      "Epoch 39/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0408 - val_loss: 0.0453\n",
      "Epoch 40/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0330 - val_loss: 0.0534\n",
      "Epoch 41/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0347 - val_loss: 0.0362\n",
      "Epoch 42/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0320 - val_loss: 0.0727\n",
      "Epoch 43/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0405 - val_loss: 0.0334\n",
      "Epoch 44/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0269 - val_loss: 0.0531\n",
      "Epoch 45/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0371 - val_loss: 0.0342\n",
      "Epoch 46/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0292 - val_loss: 0.0579\n",
      "Epoch 47/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0294 - val_loss: 0.0264\n",
      "Epoch 48/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0297 - val_loss: 0.0396\n",
      "Epoch 49/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0261 - val_loss: 0.0354\n",
      "Epoch 50/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0286 - val_loss: 0.0376\n",
      "Epoch 51/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0317 - val_loss: 0.0268\n",
      "Epoch 52/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0236 - val_loss: 0.0334\n",
      "Epoch 53/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0226 - val_loss: 0.0407\n",
      "Epoch 54/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0224 - val_loss: 0.0160\n",
      "Epoch 55/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0243 - val_loss: 0.0664\n",
      "Epoch 56/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0336 - val_loss: 0.0140\n",
      "Epoch 57/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0183 - val_loss: 0.0443\n",
      "Epoch 58/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0292 - val_loss: 0.0206\n",
      "Epoch 59/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0257 - val_loss: 0.0481\n",
      "Epoch 60/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0246 - val_loss: 0.0161\n",
      "Epoch 61/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0218 - val_loss: 0.0584\n",
      "Epoch 62/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0315 - val_loss: 0.0133\n",
      "Epoch 63/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0266 - val_loss: 0.0331\n",
      "Epoch 64/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0230 - val_loss: 0.0205\n",
      "Epoch 65/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0185 - val_loss: 0.0178\n",
      "Epoch 66/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0169 - val_loss: 0.0284\n",
      "Epoch 67/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0213 - val_loss: 0.0234\n",
      "Epoch 68/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0221 - val_loss: 0.0211\n",
      "Epoch 69/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0187 - val_loss: 0.0289\n",
      "Epoch 70/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0241 - val_loss: 0.0153\n",
      "Epoch 71/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0190 - val_loss: 0.0347\n",
      "Epoch 72/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0244 - val_loss: 0.0154\n",
      "Epoch 73/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0148 - val_loss: 0.0468\n",
      "Epoch 74/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0236 - val_loss: 0.0211\n",
      "Epoch 75/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0163 - val_loss: 0.0350\n",
      "Epoch 76/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0156 - val_loss: 0.0178\n",
      "Epoch 77/150\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0226 - val_loss: 0.0335\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 508ms/step\n",
      "Predicted demand for tomorrow: 366.54 MW\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    " \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    " \n",
    "df = pd.read_csv(\"Datasets/electricity_daily_avg.csv\").sort_values(\"Day\").reset_index(drop=True)\n",
    "target_col = \"Electricity Demand (MW)\"  \n",
    " \n",
    "series = df[target_col].to_numpy(dtype=np.float32).reshape(-1, 1)\n",
    " \n",
    "N = len(series)\n",
    "train_size = int(N * 0.70)\n",
    "val_size = int(N * 0.15)\n",
    " \n",
    "train = series[:train_size]\n",
    "val   = series[train_size:train_size + val_size]\n",
    "test  = series[train_size + val_size:]\n",
    " \n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train)\n",
    "val_scaled   = scaler.transform(val)\n",
    "test_scaled  = scaler.transform(test)\n",
    " \n",
    "def make_sequences(data, look_back):\n",
    "    data = np.asarray(data)\n",
    "    if data.ndim == 1:\n",
    "        data = data.reshape(-1, 1)\n",
    " \n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - look_back):\n",
    "        X.append(data[i:i+look_back, :])  \n",
    "        y.append(data[i+look_back, 0])    \n",
    " \n",
    "    X = np.array(X, dtype=np.float32)      \n",
    "    y = np.array(y, dtype=np.float32)        \n",
    "    return X, y\n",
    " \n",
    "LOOK_BACK = 7\n",
    "X_train, y_train = make_sequences(train_scaled, LOOK_BACK)\n",
    "X_val, y_val     = make_sequences(val_scaled, LOOK_BACK)\n",
    "X_test, y_test   = make_sequences(test_scaled, LOOK_BACK)\n",
    " \n",
    "print(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
    "print(\"X_val  :\", X_val.shape,   \"y_val  :\", y_val.shape)\n",
    "print(\"X_test :\", X_test.shape,  \"y_test :\", y_test.shape)\n",
    " \n",
    "tf.keras.utils.set_random_seed(42)\n",
    " \n",
    " \n",
    "model = Sequential([\n",
    "    GRU(64, return_sequences=True, input_shape=(LOOK_BACK, 1)),\n",
    "    Dropout(0.2),\n",
    " \n",
    "    LSTM(32, return_sequences=False),   # <- LSTM added here\n",
    "    Dropout(0.2),\n",
    " \n",
    "    Dense(1)\n",
    "])\n",
    " \n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    " \n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=150,\n",
    "    batch_size=16,\n",
    "    callbacks=[EarlyStopping(patience=15, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")\n",
    " \n",
    " \n",
    "full_scaled = scaler.transform(series)  \n",
    "last_seq = full_scaled[-LOOK_BACK:].reshape(1, LOOK_BACK, 1)\n",
    "tomorrow_scaled = model.predict(last_seq)\n",
    "tomorrow_mw = scaler.inverse_transform(tomorrow_scaled)[0, 0]\n",
    " \n",
    "print(f\"Predicted demand for tomorrow: {tomorrow_mw:.2f} MW\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e52c131",
   "metadata": {},
   "source": [
    "ğŸ“Œ Business Scenario\n",
    "A smart city power department wants to predict tomorrowâ€™s electricity demand (in MW) using the last 7 days of hourly consumption data.\n",
    "This helps to:\n",
    "Avoid power shortages âš¡\n",
    "Optimize power generation scheduling\n",
    "Reduce operational costs\n",
    "ğŸ”¹ Sample Dataset (Simplified â€“ Daily Average)\n",
    " \n",
    "Day\tElectricity Demand (MW)\n",
    "1\t320\n",
    "2\t340\n",
    "3\t360\n",
    "4\t355\n",
    "5\t370\n",
    "6\t390\n",
    "7\t410\n",
    "8\tTarget (to predict)\n",
    " \n",
    " \n",
    "Use LSTM and GRU model for forcasting\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa69e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Siddharth.Kshirsagar\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_11\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_11\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,352</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚         \u001b[38;5;34m4,352\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_19 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m33\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,385</span> (17.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,385\u001b[0m (17.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,385</span> (17.13 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,385\u001b[0m (17.13 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_12\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_12\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ gru_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,360</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ gru_5 (\u001b[38;5;33mGRU\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚         \u001b[38;5;34m3,360\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_20 (\u001b[38;5;33mDense\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m33\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,393</span> (13.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,393\u001b[0m (13.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,393</span> (13.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,393\u001b[0m (13.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM final train loss: 105510.5390625\n",
      "GRU  final train loss: 107403.5\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000029D0179AAF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000029D0179AD30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense\n",
    "\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "series = np.array([320, 340, 360, 345, 370, 390, 410], dtype=np.float32)\n",
    "\n",
    "window_size = 7  \n",
    "assert len(series) == window_size, \"We only have the 7 days that form a single training window.\"\n",
    "\n",
    "\n",
    "X = series.reshape(1, window_size, 1)  # shape: (batch=1, time=7, features=1)\n",
    "\n",
    "last_diff = float(series[-1] - series[-2])  # 410 - 390 = 20\n",
    "y_value = float(series[-1] + last_diff)     \n",
    "y = np.array([[y_value]], dtype=np.float32)  \n",
    "\n",
    "\n",
    "x_mean = X.mean()\n",
    "x_std  = X.std() + 1e-8\n",
    "Xn = (X - x_mean) / x_std\n",
    "\n",
    "\n",
    "def build_lstm():\n",
    "    model = Sequential([\n",
    "        LSTM(32, input_shape=(window_size, 1)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2, clipnorm=1.0),\n",
    "                  loss='mse')\n",
    "    return model\n",
    "\n",
    "def build_gru():\n",
    "    model = Sequential([\n",
    "        GRU(32, input_shape=(window_size, 1)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2, clipnorm=1.0),\n",
    "                  loss='mse')\n",
    "    return model\n",
    "\n",
    "lstm_model = build_lstm()\n",
    "gru_model  = build_gru()\n",
    "\n",
    "lstm_model.summary()\n",
    "gru_model.summary()\n",
    "\n",
    "\n",
    "epochs = 300\n",
    "history_lstm = lstm_model.fit(Xn, y, epochs=epochs, batch_size=1, verbose=0)\n",
    "history_gru  = gru_model.fit( Xn, y, epochs=epochs, batch_size=1, verbose=0)\n",
    "\n",
    "print(\"LSTM final train loss:\", float(history_lstm.history['loss'][-1]))\n",
    "print(\"GRU  final train loss:\", float(history_gru.history['loss'][-1]))\n",
    "\n",
    "\n",
    "pred_lstm = float(lstm_model.predict(Xn, verbose=0)[0, 0])\n",
    "pred_gru  = float(gru_model.predict( Xn, verbose=0)[0, 0])\n",
    "\n",
    "naive_repeat_last = float(series[-1])          \n",
    "naive_add_last_diff = float(series[-1] + last_diff)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
